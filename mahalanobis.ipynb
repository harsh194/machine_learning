{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KBvQhI4Ud6DxIoalhLkSOi5sHQ_gZp3g",
      "authorship_tag": "ABX9TyPK6hE7+oiBJK9b5/1Ipygl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh194/machine_learning/blob/main/mahalanobis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YW6GXyfXzlxF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "new_directory = \"/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/\"\n",
        "os.chdir(new_directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.path)\n",
        "import os\n",
        "os.add(\"/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/\")"
      ],
      "metadata": {
        "id": "4d2vmvTg157u",
        "outputId": "0202bdff-011f-4548-be56-c445a500ba98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.10/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-52cf96c57f20>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'add'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install EfficientNet_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0uOsVkv3V1z",
        "outputId": "f95bf058-8a60-4aa6-ff0a-a7945579ddd2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: EfficientNet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from EfficientNet_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->EfficientNet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->EfficientNet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->EfficientNet_pytorch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "import argparse\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.covariance import LedoitWolf\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "import datasets.mvtec as mvtec\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms as T"
      ],
      "metadata": {
        "id": "rC7-KK8dz9pM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser('MahalanobisAD')\n",
        "    parser.add_argument(\"--model_name\", type=str, default='efficientnet-b4')\n",
        "    parser.add_argument(\"--save_path\", type=str, default=\"./result\")\n",
        "    return parser.parse_args([])"
      ],
      "metadata": {
        "id": "Gw9D3A5F0Btj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lmngcQCvDPQf",
        "outputId": "ceb91e87-16ed-4387-cd14-26eaa7a68b93"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MVTecDataset(Dataset):\n",
        "    def __init__(self, root_path='/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/', class_name='bottle', is_train=True,\n",
        "                 resize=256, cropsize=224):\n",
        "        assert class_name in CLASS_NAMES, 'class_name: {}, should be in {}'.format(class_name, CLASS_NAMES)\n",
        "        self.root_path = root_path\n",
        "        self.class_name = class_name\n",
        "        self.is_train = is_train\n",
        "        self.resize = resize\n",
        "        self.cropsize = cropsize\n",
        "#         self.mvtec_folder_path = os.path.join(root_path, 'mvtec_anomaly_detection')\n",
        "        self.mvtec_folder_path = os.path.join(root_path, 'mvtec_anomaly_detection')\n",
        "\n",
        "        # load dataset\n",
        "        self.x, self.y, self.mask = self.load_dataset_folder()\n",
        "\n",
        "        # set transforms\n",
        "        self.transform_x = T.Compose([T.Resize(resize, Image.ANTIALIAS),\n",
        "                                      T.CenterCrop(cropsize),\n",
        "                                      T.ToTensor(),\n",
        "                                      T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                  std=[0.229, 0.224, 0.225])])\n",
        "        self.transform_mask = T.Compose([T.Resize(resize, Image.NEAREST),\n",
        "                                         T.CenterCrop(cropsize),\n",
        "                                         T.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y, mask = self.x[idx], self.y[idx], self.mask[idx]\n",
        "\n",
        "        x = Image.open(x).convert('RGB')\n",
        "        x = self.transform_x(x)\n",
        "\n",
        "        if y == 0:\n",
        "            mask = torch.zeros([1, self.cropsize, self.cropsize])\n",
        "        else:\n",
        "            mask = Image.open(mask)\n",
        "            mask = self.transform_mask(mask)\n",
        "\n",
        "        return x, y, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def load_dataset_folder(self):\n",
        "        phase = 'train' if self.is_train else 'test'\n",
        "        x, y, mask = [], [], []\n",
        "\n",
        "        img_dir = os.path.join(self.mvtec_folder_path, self.class_name, phase)\n",
        "        gt_dir = os.path.join(self.mvtec_folder_path, self.class_name, 'ground_truth')\n",
        "\n",
        "        img_types = sorted(os.listdir(img_dir))\n",
        "        for img_type in img_types:\n",
        "\n",
        "            # load images\n",
        "            img_type_dir = os.path.join(img_dir, img_type)\n",
        "            if not os.path.isdir(img_type_dir):\n",
        "                continue\n",
        "            img_fpath_list = sorted([os.path.join(img_type_dir, f)\n",
        "                                     for f in os.listdir(img_type_dir)\n",
        "                                     if f.endswith('.png')])\n",
        "            x.extend(img_fpath_list)\n",
        "\n",
        "            # load gt labels\n",
        "            if img_type == 'good':\n",
        "                y.extend([0] * len(img_fpath_list))\n",
        "                mask.extend([None] * len(img_fpath_list))\n",
        "            else:\n",
        "                y.extend([1] * len(img_fpath_list))\n",
        "                gt_type_dir = os.path.join(gt_dir, img_type)\n",
        "                img_fname_list = [os.path.splitext(os.path.basename(f))[0] for f in img_fpath_list]\n",
        "                gt_fpath_list = [os.path.join(gt_type_dir, img_fname + '_mask.png')\n",
        "                                 for img_fname in img_fname_list]\n",
        "                mask.extend(gt_fpath_list)\n",
        "\n",
        "        assert len(x) == len(y), 'number of x and y should be same'\n",
        "\n",
        "        return list(x), list(y), list(mask)"
      ],
      "metadata": {
        "id": "C7AaTwD--ptm"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    args = parse_args()\n",
        "    assert args.model_name.startswith('efficientnet-b'), 'only support efficientnet variants, not %s' % args.model_name\n",
        "\n",
        "    # device setup\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # load model\n",
        "    model = EfficientNetModified.from_pretrained(args.model_name)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    os.makedirs(os.path.join(args.save_path, 'temp'), exist_ok=True)\n",
        "\n",
        "    total_roc_auc = []\n",
        "\n",
        "    for class_name in mvtec.CLASS_NAMES:\n",
        "\n",
        "        train_dataset = mvtec.MVTecDataset(class_name=class_name, is_train=True)\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=32, pin_memory=True)\n",
        "        test_dataset = mvtec.MVTecDataset(class_name=class_name, is_train=False)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=32, pin_memory=True)\n",
        "\n",
        "        train_outputs = [[] for _ in range(9)]\n",
        "        test_outputs = [[] for _ in range(9)]\n",
        "\n",
        "        # extract train set features\n",
        "        train_feat_filepath = os.path.join(args.save_path, 'temp', 'train_%s_%s.pkl' % (class_name, args.model_name))\n",
        "        if not os.path.exists(train_feat_filepath):\n",
        "            for (x, y, mask) in tqdm(train_dataloader, '| feature extraction | train | %s |' % class_name):\n",
        "                # model prediction\n",
        "                with torch.no_grad():\n",
        "                    feats = model.extract_features(x.to(device))\n",
        "                for f_idx, feat in enumerate(feats):\n",
        "                    train_outputs[f_idx].append(feat)\n",
        "\n",
        "            # fitting a multivariate gaussian to features extracted from every level of ImageNet pre-trained model\n",
        "            for t_idx, train_output in enumerate(train_outputs):\n",
        "                mean = torch.mean(torch.cat(train_output, 0).squeeze(), dim=0).cpu().detach().numpy()\n",
        "                # covariance estimation by using the Ledoit. Wolf et al. method\n",
        "                cov = LedoitWolf().fit(torch.cat(train_output, 0).squeeze().cpu().detach().numpy()).covariance_\n",
        "                train_outputs[t_idx] = [mean, cov]\n",
        "\n",
        "            # save extracted feature\n",
        "            with open(train_feat_filepath, 'wb') as f:\n",
        "                pickle.dump(train_outputs, f)\n",
        "        else:\n",
        "            print('load train set feature distribution from: %s' % train_feat_filepath)\n",
        "            with open(train_feat_filepath, 'rb') as f:\n",
        "                train_outputs = pickle.load(f)\n",
        "\n",
        "        gt_list = []\n",
        "\n",
        "        # extract test set features\n",
        "        for (x, y, mask) in tqdm(test_dataloader, '| feature extraction | test | %s |' % class_name):\n",
        "            gt_list.extend(y.cpu().detach().numpy())\n",
        "            # model prediction\n",
        "            with torch.no_grad():\n",
        "                feats = model.extract_features(x.to(device))\n",
        "            for f_idx, feat in enumerate(feats):\n",
        "                test_outputs[f_idx].append(feat)\n",
        "        for t_idx, test_output in enumerate(test_outputs):\n",
        "            test_outputs[t_idx] = torch.cat(test_output, 0).squeeze().cpu().detach().numpy()\n",
        "\n",
        "        # calculate Mahalanobis distance per each level of EfficientNet\n",
        "        dist_list = []\n",
        "        for t_idx, test_output in enumerate(test_outputs):\n",
        "            mean = train_outputs[t_idx][0]\n",
        "            cov_inv = np.linalg.inv(train_outputs[t_idx][1])\n",
        "            dist = [mahalanobis(sample, mean, cov_inv) for sample in test_output]\n",
        "            dist_list.append(np.array(dist))\n",
        "\n",
        "        # Anomaly score is followed by unweighted summation of the Mahalanobis distances\n",
        "        scores = np.sum(np.array(dist_list), axis=0)\n",
        "\n",
        "        # calculate image-level ROC AUC score\n",
        "        fpr, tpr, _ = roc_curve(gt_list, scores)\n",
        "        roc_auc = roc_auc_score(gt_list, scores)\n",
        "        total_roc_auc.append(roc_auc)\n",
        "        print('%s ROCAUC: %.3f' % (class_name, roc_auc))\n",
        "        plt.plot(fpr, tpr, label='%s ROCAUC: %.3f' % (class_name, roc_auc))\n",
        "\n",
        "    print('Average ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
        "    plt.title('Average image ROCAUC: %.3f' % np.mean(total_roc_auc))\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(os.path.join(args.save_path, 'roc_curve_%s.png' % args.model_name), dpi=200)\n"
      ],
      "metadata": {
        "id": "uj3dGplk9IH_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetModified(EfficientNet):\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns list of the feature at each level of the EfficientNet \"\"\"\n",
        "\n",
        "        feat_list = []\n",
        "\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "        feat_list.append(F.adaptive_avg_pool2d(x, 1))\n",
        "\n",
        "        # Blocks\n",
        "        x_prev = x\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if (x_prev.shape[1] != x.shape[1] and idx != 0) or idx == (len(self._blocks) - 1):\n",
        "                feat_list.append(F.adaptive_avg_pool2d(x_prev, 1))\n",
        "            x_prev = x\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "        feat_list.append(F.adaptive_avg_pool2d(x, 1))\n",
        "\n",
        "        return feat_list"
      ],
      "metadata": {
        "id": "m3nsdNOp9VOj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "lzXvr8Wu-3dP",
        "outputId": "9b2fd0f1-ffa8-46e7-82c9-75d08031f50f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c7bc734e5e35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-98d9eb4c3af5>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmvtec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLASS_NAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmvtec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMVTecDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmvtec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMVTecDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/datasets/mvtec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, class_name, is_train, resize, cropsize)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# set transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Harsh/Machine Learning/Mahalanobis/datasets/mvtec.py\u001b[0m in \u001b[0;36mload_dataset_folder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mgt_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmvtec_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ground_truth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mimg_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/mvtec_anomaly_detection/bottle/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb\n"
      ],
      "metadata": {
        "id": "DAoFKWHM-6rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zvgsy4_qAweC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}